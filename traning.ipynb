{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOV3 training example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/myx/anaconda3/envs/cs231n/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from yolo import YOLO, detect_video\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from yolo3.model import yolo_eval, yolo_body, tiny_yolo_body\n",
    "from keras.layers import Input\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Lambda\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "from yolo3.model import preprocess_true_boxes, yolo_body, tiny_yolo_body, yolo_loss\n",
    "from yolo3.utils import get_random_data\n",
    "import tensorflow as tf\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_classes(classes_path):\n",
    "    '''loads the classes'''\n",
    "    with open(classes_path) as f:\n",
    "        class_names = f.readlines()\n",
    "    class_names = [c.strip() for c in class_names]\n",
    "    return class_names\n",
    "\n",
    "def get_anchors(anchors_path):\n",
    "    '''loads the anchors from a file'''\n",
    "    with open(anchors_path) as f:\n",
    "        anchors = f.readline()\n",
    "    anchors = [float(x) for x in anchors.split(',')]\n",
    "    return np.array(anchors).reshape(-1, 2)\n",
    "\n",
    "def create_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2,\n",
    "            weights_path='model_data/yolo.h5'):\n",
    "    '''create the training model'''\n",
    "   \n",
    "    image_input = Input(shape=(None, None, 3))\n",
    "    h, w = input_shape\n",
    "    num_anchors = len(anchors)\n",
    "\n",
    "    y_true = [Input(shape=(h//{0:32, 1:16, 2:8}[l], w//{0:32, 1:16, 2:8}[l], \\\n",
    "        num_anchors//3, num_classes+5)) for l in range(3)]\n",
    "\n",
    "    model_body = yolo_body(image_input, num_anchors//3, num_classes)\n",
    "    print('Create YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes))\n",
    "\n",
    "    if load_pretrained:\n",
    "        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
    "        print('Load weights {}.'.format(weights_path))\n",
    "        if freeze_body in [1, 2]:\n",
    "            # Freeze darknet53 body or freeze all but 3 output layers.\n",
    "            num = (185, -3)[freeze_body-1]\n",
    "            for i in range(num): model_body.layers[i].trainable = False\n",
    "            print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n",
    "\n",
    "    model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss',\n",
    "        arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.5})(\n",
    "        [*model_body.output, *y_true])\n",
    "    model = Model([model_body.input, *y_true], model_loss)\n",
    "\n",
    "    return model\n",
    "\n",
    "def data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
    "    '''data generator for fit_generator'''\n",
    "    n = len(annotation_lines)\n",
    "    #print(\"annotation_lines: \", n)\n",
    "    i = 0\n",
    "    while True:\n",
    "        image_data = []\n",
    "        box_data = []\n",
    "        for b in range(batch_size):\n",
    "           # if i==0:\n",
    "               # np.random.shuffle(annotation_lines)\n",
    "            #随机裁剪、旋转、变换颜色（hue）、变换饱和度（saturation）， 变换曝光度（exposure shifts）\n",
    "            # random 为 False的时候会出错  导致loss 为 non\n",
    "            image, box = get_random_data(annotation_lines[i], input_shape, random=True)\n",
    "            image_data.append(image)\n",
    "            box_data.append(box)\n",
    "            i = (i+1) % n\n",
    "        image_data = np.array(image_data)\n",
    "        box_data = np.array(box_data)\n",
    "        # 每一个grid cell内的box是否含有object 以及如果含有的情况下的具体box信息\n",
    "        y_true = preprocess_true_boxes(box_data, input_shape, anchors, num_classes)\n",
    "        yield [image_data, *y_true], np.zeros(batch_size)\n",
    "\n",
    "def data_generator_wrapper(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
    "    n = len(annotation_lines)\n",
    "    if n==0 or batch_size<=0: return None\n",
    "    return data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes)\n",
    "\n",
    "def create_tiny_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2,\n",
    "            weights_path='model_data/yolo-tiny.h5'):\n",
    "    '''create the training model, for Tiny YOLOv3'''\n",
    "    K.clear_session() # get a new session\n",
    "    image_input = Input(shape=(None, None, 3))\n",
    "    h, w = input_shape\n",
    "    num_anchors = len(anchors)\n",
    "\n",
    "    y_true = [Input(shape=(h//{0:32, 1:16}[l], w//{0:32, 1:16}[l], \\\n",
    "        num_anchors//2, num_classes+5)) for l in range(2)]\n",
    "\n",
    "    model_body = tiny_yolo_body(image_input, num_anchors//2, num_classes)\n",
    "    print('Create Tiny YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes))\n",
    "\n",
    "    if load_pretrained:\n",
    "        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
    "        print('Load weights {}.'.format(weights_path))\n",
    "        if freeze_body in [1, 2]:\n",
    "            # Freeze the darknet body or freeze all but 2 output layers.\n",
    "            num = (20, len(model_body.layers)-2)[freeze_body-1]\n",
    "            for i in range(num): model_body.layers[i].trainable = False\n",
    "            print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n",
    "\n",
    "    model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss',\n",
    "        arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.7})(\n",
    "        [*model_body.output, *y_true])\n",
    "    model = Model([model_body.input, *y_true], model_loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create YOLOv3 model with 9 anchors and 80 classes.\n",
      "Load weights model_data/downloaded_coco.h5.\n",
      "Freeze the first -3 layers of total 252 layers.\n",
      "Training data size:  82080\n",
      "model_data/coco/images/train2014/COCO_train2014_000000480023.jpg 116,305,401,571,52 75,134,278,349,0 239,176,349,309,0 394,222,479,419,0 0,0,78,164,0 345,217,407,284,0 4,128,124,286,0 381,241,430,298,0 225,111,292,201,0 45,402,466,635,0 0,525,68,639,0 0,236,126,518,0\n",
      "\n",
      "Train on 73872 samples, val on 8208 samples, with batch size 8.\n",
      "Epoch 1/50\n",
      "9234/9234 [==============================] - 4810s 521ms/step - loss: 89.1911 - val_loss: 82.1989\n",
      "Epoch 2/50\n",
      "9234/9234 [==============================] - 4788s 518ms/step - loss: 79.2686 - val_loss: 77.3676\n",
      "Epoch 3/50\n",
      "9234/9234 [==============================] - 4807s 521ms/step - loss: 73.0290 - val_loss: 71.4181\n",
      "Epoch 4/50\n",
      "9234/9234 [==============================] - 4789s 519ms/step - loss: 69.9158 - val_loss: 71.8447\n",
      "Epoch 5/50\n",
      "9234/9234 [==============================] - 4790s 519ms/step - loss: 68.3542 - val_loss: 69.3636\n",
      "Epoch 6/50\n",
      "9234/9234 [==============================] - 4788s 519ms/step - loss: 67.2248 - val_loss: 68.0462\n",
      "Epoch 7/50\n",
      "9234/9234 [==============================] - 4789s 519ms/step - loss: 66.4953 - val_loss: 67.4289\n",
      "Epoch 8/50\n",
      "9234/9234 [==============================] - 4788s 519ms/step - loss: 66.0674 - val_loss: 66.7668\n",
      "Epoch 9/50\n",
      "9234/9234 [==============================] - 4787s 518ms/step - loss: 65.5599 - val_loss: 65.9796\n",
      "Epoch 10/50\n",
      "9234/9234 [==============================] - 4787s 518ms/step - loss: 65.3030 - val_loss: 66.1663\n",
      "Epoch 11/50\n",
      "9234/9234 [==============================] - 4790s 519ms/step - loss: 65.1169 - val_loss: 65.7811\n",
      "Epoch 12/50\n",
      "9234/9234 [==============================] - 4791s 519ms/step - loss: 64.8815 - val_loss: 65.8511\n",
      "Epoch 13/50\n",
      "9234/9234 [==============================] - 4789s 519ms/step - loss: 64.6333 - val_loss: 65.5033\n",
      "Epoch 14/50\n",
      "9234/9234 [==============================] - 4791s 519ms/step - loss: 64.6068 - val_loss: 65.8400\n",
      "Epoch 15/50\n",
      "9234/9234 [==============================] - 4790s 519ms/step - loss: 64.4054 - val_loss: 65.9523\n",
      "Epoch 16/50\n",
      "9234/9234 [==============================] - 4790s 519ms/step - loss: 64.2955 - val_loss: 65.4169\n",
      "Epoch 17/50\n",
      "9234/9234 [==============================] - 4784s 518ms/step - loss: 64.1727 - val_loss: 65.3078\n",
      "Epoch 18/50\n",
      "9234/9234 [==============================] - 4790s 519ms/step - loss: 63.9577 - val_loss: 64.7921\n",
      "Epoch 19/50\n",
      "9234/9234 [==============================] - 4789s 519ms/step - loss: 63.9467 - val_loss: 63.7200\n",
      "Epoch 20/50\n",
      "9234/9234 [==============================] - 4791s 519ms/step - loss: 63.8211 - val_loss: 64.0377\n",
      "Epoch 21/50\n",
      "9234/9234 [==============================] - 4791s 519ms/step - loss: 63.8460 - val_loss: 63.5721\n",
      "Epoch 22/50\n",
      "9234/9234 [==============================] - 4793s 519ms/step - loss: 63.6589 - val_loss: 64.7731\n",
      "Epoch 23/50\n",
      "9234/9234 [==============================] - 4792s 519ms/step - loss: 63.5909 - val_loss: 64.0332\n",
      "Epoch 24/50\n",
      "9234/9234 [==============================] - 4790s 519ms/step - loss: 63.5608 - val_loss: 64.2900\n",
      "Epoch 25/50\n",
      "9234/9234 [==============================] - 4785s 518ms/step - loss: 63.4703 - val_loss: 64.8340\n",
      "Epoch 26/50\n",
      "9234/9234 [==============================] - 4786s 518ms/step - loss: 63.4392 - val_loss: 65.2876\n",
      "Epoch 27/50\n",
      "9234/9234 [==============================] - 4789s 519ms/step - loss: 63.3109 - val_loss: 64.8044\n",
      "Epoch 28/50\n",
      "9234/9234 [==============================] - 4790s 519ms/step - loss: 63.2881 - val_loss: 64.2183\n",
      "Epoch 29/50\n",
      "9234/9234 [==============================] - 4789s 519ms/step - loss: 63.1964 - val_loss: 64.5209\n",
      "Epoch 30/50\n",
      "9234/9234 [==============================] - 4788s 519ms/step - loss: 63.1375 - val_loss: 65.3345\n",
      "Epoch 31/50\n",
      "9234/9234 [==============================] - 4790s 519ms/step - loss: 63.1854 - val_loss: 63.7657\n",
      "Epoch 32/50\n",
      "9234/9234 [==============================] - 4792s 519ms/step - loss: 63.0938 - val_loss: 63.9504\n",
      "Epoch 33/50\n",
      "9234/9234 [==============================] - 4791s 519ms/step - loss: 63.1095 - val_loss: 64.3593\n",
      "Epoch 34/50\n",
      "9234/9234 [==============================] - 4791s 519ms/step - loss: 63.0644 - val_loss: 63.6853\n",
      "Epoch 35/50\n",
      "9234/9234 [==============================] - 4792s 519ms/step - loss: 63.0368 - val_loss: 64.1262\n",
      "Epoch 36/50\n",
      "9234/9234 [==============================] - 4794s 519ms/step - loss: 63.0707 - val_loss: 64.3191\n",
      "Epoch 37/50\n",
      "9234/9234 [==============================] - 4795s 519ms/step - loss: 63.0057 - val_loss: 64.9126\n",
      "Epoch 38/50\n",
      "9234/9234 [==============================] - 4791s 519ms/step - loss: 62.8825 - val_loss: 63.6025\n",
      "Epoch 39/50\n",
      "9234/9234 [==============================] - 4786s 518ms/step - loss: 62.8873 - val_loss: 63.2737\n",
      "Epoch 40/50\n",
      "9234/9234 [==============================] - 4792s 519ms/step - loss: 62.8174 - val_loss: 63.6302\n",
      "Epoch 41/50\n",
      "9234/9234 [==============================] - 4793s 519ms/step - loss: 62.7541 - val_loss: 64.4189\n",
      "Epoch 42/50\n",
      "9234/9234 [==============================] - 4791s 519ms/step - loss: 62.8346 - val_loss: 66.0305\n",
      "Epoch 43/50\n",
      "9234/9234 [==============================] - 4793s 519ms/step - loss: 62.7875 - val_loss: 62.7522\n",
      "Epoch 44/50\n",
      "9234/9234 [==============================] - 4789s 519ms/step - loss: 62.8526 - val_loss: 63.3258\n",
      "Epoch 45/50\n",
      "9234/9234 [==============================] - 4791s 519ms/step - loss: 62.7804 - val_loss: 63.3443\n",
      "Epoch 46/50\n",
      "9234/9234 [==============================] - 4791s 519ms/step - loss: 62.6952 - val_loss: 62.8413\n",
      "Epoch 47/50\n",
      "9234/9234 [==============================] - 4791s 519ms/step - loss: 62.7077 - val_loss: 63.0042\n",
      "Epoch 48/50\n",
      "9234/9234 [==============================] - 4785s 518ms/step - loss: 62.7080 - val_loss: 64.8136\n",
      "Epoch 49/50\n",
      "9234/9234 [==============================] - 4789s 519ms/step - loss: 62.6545 - val_loss: 63.8783\n",
      "Epoch 50/50\n",
      "9234/9234 [==============================] - 4789s 519ms/step - loss: 62.6369 - val_loss: 63.4110\n",
      "Unfreeze all of the layers.\n",
      "Train on 73872 samples, val on 8208 samples, with batch size 8.\n",
      "Epoch 51/100\n",
      "9234/9234 [==============================] - 4832s 523ms/step - loss: 58.9383 - val_loss: 58.0729\n",
      "Epoch 52/100\n",
      "9234/9234 [==============================] - 4824s 522ms/step - loss: 57.8952 - val_loss: 57.2149\n",
      "Epoch 53/100\n",
      "9234/9234 [==============================] - 4822s 522ms/step - loss: 57.4986 - val_loss: 57.1084\n",
      "Epoch 54/100\n",
      "9234/9234 [==============================] - 4824s 522ms/step - loss: 57.0162 - val_loss: 56.6665\n",
      "Epoch 55/100\n",
      "9234/9234 [==============================] - 4820s 522ms/step - loss: 56.7119 - val_loss: 56.4630\n",
      "Epoch 56/100\n",
      "9234/9234 [==============================] - 4821s 522ms/step - loss: 56.4655 - val_loss: 56.5213\n",
      "Epoch 57/100\n",
      "9234/9234 [==============================] - 4823s 522ms/step - loss: 56.3319 - val_loss: 56.0569\n",
      "Epoch 58/100\n",
      "9234/9234 [==============================] - 4822s 522ms/step - loss: 56.0451 - val_loss: 55.9969\n",
      "Epoch 59/100\n",
      "9234/9234 [==============================] - 4822s 522ms/step - loss: 55.8450 - val_loss: 55.3353\n",
      "Epoch 60/100\n",
      "9234/9234 [==============================] - 4821s 522ms/step - loss: 55.5969 - val_loss: 55.3880\n",
      "Epoch 61/100\n",
      "9234/9234 [==============================] - 4820s 522ms/step - loss: 55.5164 - val_loss: 55.3263\n",
      "Epoch 62/100\n",
      "9234/9234 [==============================] - 4821s 522ms/step - loss: 55.4066 - val_loss: 55.3995\n",
      "Epoch 63/100\n",
      "9234/9234 [==============================] - 4823s 522ms/step - loss: 55.2678 - val_loss: 55.2192\n",
      "Epoch 64/100\n",
      "9234/9234 [==============================] - 4820s 522ms/step - loss: 55.1653 - val_loss: 54.8081\n",
      "Epoch 65/100\n",
      "9234/9234 [==============================] - 4825s 522ms/step - loss: 55.0270 - val_loss: 54.9339\n",
      "Epoch 66/100\n",
      "9234/9234 [==============================] - 4820s 522ms/step - loss: 54.9402 - val_loss: 54.9665\n",
      "Epoch 67/100\n",
      "9234/9234 [==============================] - 4825s 522ms/step - loss: 55.0078 - val_loss: 54.8058\n",
      "Epoch 68/100\n",
      "9234/9234 [==============================] - 4823s 522ms/step - loss: 54.7944 - val_loss: 54.6653\n",
      "Epoch 69/100\n",
      "9234/9234 [==============================] - 4824s 522ms/step - loss: 54.6693 - val_loss: 54.3031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/100\n",
      "9234/9234 [==============================] - 4825s 523ms/step - loss: 54.6709 - val_loss: 54.3391\n",
      "Epoch 71/100\n",
      "9234/9234 [==============================] - 4818s 522ms/step - loss: 54.5049 - val_loss: 54.3035\n",
      "Epoch 72/100\n",
      "9234/9234 [==============================] - 4825s 522ms/step - loss: 54.3289 - val_loss: 54.3433\n",
      "Epoch 73/100\n",
      "9234/9234 [==============================] - 4823s 522ms/step - loss: 54.4074 - val_loss: 54.2569\n",
      "Epoch 74/100\n",
      "9234/9234 [==============================] - 4823s 522ms/step - loss: 54.2760 - val_loss: 54.2430\n",
      "Epoch 75/100\n",
      "9234/9234 [==============================] - 4824s 522ms/step - loss: 54.1004 - val_loss: 53.9496\n",
      "Epoch 76/100\n",
      "9234/9234 [==============================] - 4818s 522ms/step - loss: 54.1495 - val_loss: 54.2083\n",
      "Epoch 77/100\n",
      "9234/9234 [==============================] - 4823s 522ms/step - loss: 54.1353 - val_loss: 53.7250\n",
      "Epoch 78/100\n",
      "9234/9234 [==============================] - 4823s 522ms/step - loss: 54.0204 - val_loss: 53.9343\n",
      "Epoch 79/100\n",
      "9234/9234 [==============================] - 4825s 522ms/step - loss: 53.9693 - val_loss: 53.7696\n",
      "Epoch 80/100\n",
      "9234/9234 [==============================] - 4824s 522ms/step - loss: 53.8817 - val_loss: 53.9256\n",
      "Epoch 81/100\n",
      "9234/9234 [==============================] - 4825s 523ms/step - loss: 53.8758 - val_loss: 53.6544\n",
      "Epoch 82/100\n",
      "9234/9234 [==============================] - 4824s 522ms/step - loss: 53.9464 - val_loss: 53.7643\n",
      "Epoch 83/100\n",
      "9234/9234 [==============================] - 4825s 523ms/step - loss: 53.6983 - val_loss: 53.9883\n",
      "Epoch 84/100\n",
      "9234/9234 [==============================] - 4824s 522ms/step - loss: 53.7349 - val_loss: 53.6929\n",
      "Epoch 85/100\n",
      "9234/9234 [==============================] - 4825s 523ms/step - loss: 53.7089 - val_loss: 53.5401\n",
      "Epoch 86/100\n",
      "9234/9234 [==============================] - 4826s 523ms/step - loss: 53.5751 - val_loss: 53.4581\n",
      "Epoch 87/100\n",
      "9234/9234 [==============================] - 4827s 523ms/step - loss: 53.6289 - val_loss: 53.6096\n",
      "Epoch 88/100\n",
      "9234/9234 [==============================] - 4827s 523ms/step - loss: 53.5951 - val_loss: 53.4984\n",
      "Epoch 89/100\n",
      "9234/9234 [==============================] - 4825s 523ms/step - loss: 53.4243 - val_loss: 53.5717\n",
      "Epoch 90/100\n",
      "9234/9234 [==============================] - 4826s 523ms/step - loss: 53.5201 - val_loss: 53.4217\n",
      "Epoch 91/100\n",
      "9234/9234 [==============================] - 4826s 523ms/step - loss: 53.3587 - val_loss: 53.6841\n",
      "Epoch 92/100\n",
      "9234/9234 [==============================] - 4823s 522ms/step - loss: 53.4083 - val_loss: 53.5500\n",
      "Epoch 93/100\n",
      "9234/9234 [==============================] - 4825s 523ms/step - loss: 53.3884 - val_loss: 53.1911\n",
      "Epoch 94/100\n",
      "9234/9234 [==============================] - 4823s 522ms/step - loss: 53.3204 - val_loss: 53.2650\n",
      "Epoch 95/100\n",
      "9234/9234 [==============================] - 4823s 522ms/step - loss: 53.3283 - val_loss: 53.1785\n",
      "Epoch 96/100\n",
      "9234/9234 [==============================] - 4824s 522ms/step - loss: 53.2149 - val_loss: 53.2983\n",
      "Epoch 97/100\n",
      "9234/9234 [==============================] - 4825s 523ms/step - loss: 53.3087 - val_loss: 53.3617\n",
      "Epoch 98/100\n",
      "9234/9234 [==============================] - 4823s 522ms/step - loss: 53.2629 - val_loss: 53.2132\n",
      "Epoch 99/100\n",
      "9233/9234 [============================>.] - ETA: 0s - loss: 53.2972\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "9234/9234 [==============================] - 4825s 522ms/step - loss: 53.3003 - val_loss: 53.3152\n",
      "Epoch 100/100\n",
      "9234/9234 [==============================] - 4820s 522ms/step - loss: 52.4502 - val_loss: 52.4526\n"
     ]
    }
   ],
   "source": [
    "annotation_path = 'model_data/train_coco.txt'\n",
    "log_dir = 'logs/000/'\n",
    "classes_path = 'model_data/coco_classes.txt'\n",
    "anchors_path = 'model_data/yolo_anchors.txt'\n",
    "class_names = get_classes(classes_path)\n",
    "num_classes = len(class_names)\n",
    "anchors = get_anchors(anchors_path)\n",
    "\n",
    "input_shape = (416,416) # multiple of 32, hw\n",
    "\n",
    "is_tiny_version = len(anchors)==6 # default setting\n",
    "if is_tiny_version:\n",
    "    model = create_tiny_model(input_shape, anchors, num_classes,        \n",
    "                              freeze_body=2, weights_path='model_data/downloaded_coco_tiny.h5')\n",
    "else:\n",
    "    model = create_model(input_shape, anchors, num_classes,\n",
    "                         freeze_body=2, weights_path='model_data/downloaded_coco.h5') # make sure you know what you freeze\n",
    "\n",
    "\n",
    "\n",
    "logging = TensorBoard(log_dir=log_dir)\n",
    "checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',\n",
    "                             monitor='val_loss', save_weights_only=True, save_best_only=True, period=3)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)\n",
    "\n",
    "\n",
    "with open(annotation_path) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "#lines = lines[:10000]\n",
    "print (\"Training data size: \", len(lines))\n",
    "print(lines[0])\n",
    "\n",
    "np.random.seed(10101)\n",
    "np.random.shuffle(lines)\n",
    "np.random.seed(None)\n",
    "val_split = 0.1\n",
    "num_val = int(len(lines)*val_split)\n",
    "num_train = len(lines) - num_val\n",
    "\n",
    "# add gpu growth flags\n",
    "#tf_config.gpu_options.allow_growth = True\n",
    "#tf_config.gpu_options.per_process_gpu_memory_fraction = 0.1\n",
    "#model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "if True:\n",
    "    model.compile(optimizer=Adam(lr=1e-3), loss={\n",
    "            # use custom yolo_loss Lambda layer.\n",
    "         'yolo_loss': lambda y_true, y_pred: y_pred})\n",
    "\n",
    "    batch_size = 8\n",
    "    print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
    "    model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
    "            steps_per_epoch=max(1, num_train//batch_size),\n",
    "            validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
    "            validation_steps=max(1, num_val//batch_size),\n",
    "            epochs=50,\n",
    "            initial_epoch=0,\n",
    "            callbacks=[logging, checkpoint])\n",
    "    model.save_weights(log_dir + 'trained_weights_stage_1.h5')\n",
    "\n",
    "    # Unfreeze and continue training, to fine-tune.\n",
    "    # Train longer if the result is not good.\n",
    "if True:\n",
    "    for i in range(len(model.layers)):\n",
    "        model.layers[i].trainable = True\n",
    "    model.compile(optimizer=Adam(lr=1e-4), loss={'yolo_loss': lambda y_true, y_pred: y_pred}) # recompile to apply the change\n",
    "    print('Unfreeze all of the layers.')\n",
    "\n",
    "    batch_size = 8 # note that more GPU memory is required after unfreezing the body\n",
    "    print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
    "    model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
    "        steps_per_epoch=max(1, num_train//batch_size),\n",
    "        validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
    "        validation_steps=max(1, num_val//batch_size),\n",
    "        epochs=100,\n",
    "        initial_epoch=50,\n",
    "        callbacks=[logging, checkpoint, reduce_lr, early_stopping])\n",
    "    model.save_weights(log_dir + 'trained_weights_final.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
